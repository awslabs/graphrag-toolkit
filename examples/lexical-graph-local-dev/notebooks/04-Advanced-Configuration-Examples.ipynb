{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb1535a",
   "metadata": {},
   "source": [
    "# 04 - Advanced Configuration Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f529c1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.neo4j_graph_store_factory import Neo4jGraphStoreFactory\n",
    "\n",
    "# Register Neo4j as the graph store backend\n",
    "GraphStoreFactory.register(Neo4jGraphStoreFactory)\n",
    "\n",
    "# Initialize graph and vector stores from environment configuration\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "# Create the lexical graph index\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9071b7ea-29e2-4846-a31c-38d8efb738d8",
   "metadata": {},
   "source": [
    "## Advanced Configuration Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch_processing",
   "metadata": {},
   "source": [
    "### Batch Processing Multiple File Types from Multiple Sources\n",
    "\n",
    "You can combine multiple reader providers to process different file types in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import (\n",
    "    StructuredDataReaderProvider, StructuredDataReaderConfig,\n",
    "    MarkdownReaderProvider, MarkdownReaderConfig\n",
    ")\n",
    "\n",
    "# Initialize all readers (now with S3 support)\n",
    "readers = {\n",
    "    '.csv': StructuredDataReaderProvider(StructuredDataReaderConfig(\n",
    "        pandas_config={\"sep\": \",\"},\n",
    "        metadata_fn=lambda path: {'source': 'csv', 'file_path': path}\n",
    "    )),\n",
    "    '.json': StructuredDataReaderProvider(StructuredDataReaderConfig(\n",
    "        metadata_fn=lambda path: {'source': 'json', 'file_path': path}\n",
    "    )),\n",
    "    '.xlsx': StructuredDataReaderProvider(StructuredDataReaderConfig(\n",
    "        metadata_fn=lambda path: {'source': 'excel', 'file_path': path}\n",
    "    )),\n",
    "    '.md': MarkdownReaderProvider(MarkdownReaderConfig(\n",
    "        metadata_fn=lambda path: {'source': 'markdown', 'file_path': path}\n",
    "    ))\n",
    "}\n",
    "\n",
    "# Define file sources (mix of local and S3)\n",
    "file_sources = [\n",
    "    # Local files\n",
    "    'artifacts/sample.csv',\n",
    "    'artifacts/sample.md',\n",
    "    # S3 files\n",
    "    's3://config-test-bucket-188967239867/artifacts/sample.json',\n",
    "    's3://config-test-bucket-188967239867/artifacts/sample.xlsx'\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for file_path in file_sources:\n",
    "    # Get file extension\n",
    "    if file_path.startswith('s3://'):\n",
    "        file_ext = '.' + file_path.split('.')[-1].lower()\n",
    "    else:\n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "    \n",
    "    if file_ext in readers:\n",
    "        try:\n",
    "            source_type = 's3' if file_path.startswith('s3://') else 'local'\n",
    "            print(f\"Processing {file_path} ({source_type}) with {file_ext} reader...\")\n",
    "            docs = readers[file_ext].read(file_path)\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"  Loaded {len(docs)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"  Skipping {file_path} - unsupported file type: {file_ext}\")\n",
    "\n",
    "if all_docs:\n",
    "    print(f\"\\nTotal documents (chunks) loaded: {len(all_docs)}\")\n",
    "    print(\"Document sources:\")\n",
    "    sources = {}\n",
    "    for doc in all_docs:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        sources[source] = sources.get(source, 0) + 1\n",
    "    \n",
    "    for source, count in sources.items():\n",
    "        print(f\"  {source}: {count} documents\")\n",
    "    \n",
    "    # Index all documents together\n",
    "    print(\"\\nIndexing all documents...\")\n",
    "    graph_index.extract_and_build(all_docs, show_progress=True)\n",
    "else:\n",
    "    print(\"No supported documents found\")\n",
    "    print(\"Supported file types: .csv, .json, .xlsx, .md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configuration_examples",
   "metadata": {},
   "source": [
    "\n",
    "### Custom Metadata Functions\n",
    "\n",
    "You can create sophisticated metadata functions to enrich your documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import (\n",
    "    StructuredDataReaderProvider, StructuredDataReaderConfig,\n",
    "    MarkdownReaderProvider, MarkdownReaderConfig\n",
    ")\n",
    "\n",
    "def advanced_file_metadata(file_path):\n",
    "    \"\"\"Extract detailed metadata from file path and system info.\"\"\"\n",
    "    path = Path(file_path)\n",
    "    \n",
    "    # Check if it's an S3 path\n",
    "    if str(file_path).startswith('s3://'):\n",
    "        return s3_metadata(file_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'source': 'file',\n",
    "        'file_path': str(path),\n",
    "        'file_name': path.name,\n",
    "        'file_extension': path.suffix,\n",
    "        'file_size': path.stat().st_size if path.exists() else 0,\n",
    "        'created_date': datetime.datetime.fromtimestamp(path.stat().st_ctime).isoformat() if path.exists() else None,\n",
    "        'modified_date': datetime.datetime.fromtimestamp(path.stat().st_mtime).isoformat() if path.exists() else None,\n",
    "        'processing_date': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def youtube_metadata(video_url):\n",
    "    \"\"\"Extract metadata from YouTube URL.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Extract video ID from URL\n",
    "    video_id_match = re.search(r'(?:v=|/)([0-9A-Za-z_-]{11}).*', video_url)\n",
    "    video_id = video_id_match.group(1) if video_id_match else 'unknown'\n",
    "    \n",
    "    return {\n",
    "        'source': 'youtube',\n",
    "        'video_url': video_url,\n",
    "        'video_id': video_id,\n",
    "        'content_type': 'transcript',\n",
    "        'platform': 'youtube',\n",
    "        'processing_date': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def s3_metadata(s3_path):\n",
    "    \"\"\"Extract metadata from S3 path.\"\"\"\n",
    "    path = Path(s3_path)\n",
    "    return {\n",
    "        'source': 's3',\n",
    "        's3_path': s3_path,\n",
    "        'file_name': path.name,\n",
    "        'file_extension': path.suffix,\n",
    "        'storage_type': 'cloud',\n",
    "        'provider': 'aws',\n",
    "        'processing_date': datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Updated readers with advanced metadata\n",
    "readers = {\n",
    "    '.csv': StructuredDataReaderProvider(StructuredDataReaderConfig(\n",
    "        pandas_config={\"sep\": \",\"},\n",
    "        metadata_fn=advanced_file_metadata\n",
    "    )),\n",
    "    '.json': StructuredDataReaderProvider(StructuredDataReaderConfig(\n",
    "        metadata_fn=advanced_file_metadata\n",
    "    )),\n",
    "    '.xlsx': StructuredDataReaderProvider(StructuredDataReaderConfig(\n",
    "        metadata_fn=advanced_file_metadata\n",
    "    )),\n",
    "    '.md': MarkdownReaderProvider(MarkdownReaderConfig(\n",
    "        metadata_fn=advanced_file_metadata\n",
    "    ))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a51fcf-26f3-498c-851f-dc7d94e98ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file sources (mix of local and S3)\n",
    "file_sources = [\n",
    "    # Local files\n",
    "    'artifacts/sample.csv',\n",
    "    'artifacts/sample.md',\n",
    "    # S3 files\n",
    "    's3://config-test-bucket-188967239867/artifacts/sample.json',\n",
    "    's3://config-test-bucket-188967239867/artifacts/sample.xlsx'\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for file_path in file_sources:\n",
    "    # Get file extension\n",
    "    if file_path.startswith('s3://'):\n",
    "        file_ext = '.' + file_path.split('.')[-1].lower()\n",
    "    else:\n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "    \n",
    "    if file_ext in readers:\n",
    "        try:\n",
    "            source_type = 's3' if file_path.startswith('s3://') else 'local'\n",
    "            print(f\"Processing {file_path} ({source_type}) with {file_ext} reader...\")\n",
    "            docs = readers[file_ext].read(file_path)\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"  Loaded {len(docs)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"  Skipping {file_path} - unsupported file type: {file_ext}\")\n",
    "\n",
    "# Demonstrate rich metadata\n",
    "if all_docs:\n",
    "    print(f\"\\nTotal documents loaded: {len(all_docs)}\")\n",
    "    \n",
    "    # Show detailed metadata for first document from each source\n",
    "    print(\"\\nDetailed metadata examples:\")\n",
    "    seen_sources = set()\n",
    "    for doc in all_docs:\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        if source not in seen_sources:\n",
    "            print(f\"\\n{source.upper()} file metadata:\")\n",
    "            for key, value in doc.metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            seen_sources.add(source)\n",
    "    \n",
    "    # Index all documents\n",
    "    print(\"\\nIndexing all documents...\")\n",
    "    graph_index.extract_and_build(all_docs, show_progress=True)\n",
    "else:\n",
    "    print(\"No supported documents found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completion",
   "metadata": {},
   "source": [
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Flexible Configuration**: Each provider supports custom metadata functions\n",
    "2. **Error Handling**: Robust error handling for missing files or network issues\n",
    "3. **Batch Processing**: Ability to process multiple file types together\n",
    "4. **Cloud Integration**: Native AWS S3 support with proper credential handling\n",
    "5. **Rich Metadata**: Support for extracting detailed file and content metadata\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Place sample files in the `artifacts/` directory to test the file readers\n",
    "- Configure AWS credentials to test the S3 reader\n",
    "- Try different YouTube videos with captions for transcript extraction\n",
    "- Experiment with custom metadata functions for your specific use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3eb2e-2ebd-4770-b54d-c02a7cdd2c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
