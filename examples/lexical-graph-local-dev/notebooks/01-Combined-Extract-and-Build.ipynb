{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb1535a",
   "metadata": {},
   "source": [
    "# 01 - Combined Extract and Build using extract providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f529c1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you haven't already, install the toolkit and dependencies using the [Setup](./00-Setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa94cc9",
   "metadata": {},
   "source": [
    "## Continuous ingest - Using native llama readers\n",
    "\n",
    "See [Continous ingest](https://github.com/awslabs/graphrag-toolkit/blob/main/docs/lexical-graph/indexing.md#continous-ingest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec68542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.neo4j_graph_store_factory import Neo4jGraphStoreFactory\n",
    "\n",
    "# Suppress noisy warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Removing unpickleable private attribute.*\")\n",
    "logging.getLogger(\"neo4j.notifications\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"botocore.tokens\").setLevel(logging.ERROR)\n",
    "\n",
    "GraphStoreFactory.register(Neo4jGraphStoreFactory)\n",
    "\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store, \n",
    "    vector_store\n",
    ")\n",
    "\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/neptune/latest/userguide/intro.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-features.html',\n",
    "    'https://docs.aws.amazon.com/neptune-analytics/latest/userguide/neptune-analytics-vs-neptune-database.html'\n",
    "]\n",
    "\n",
    "docs = SimpleWebPageReader(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url:{'url': url}\n",
    ").load_data(doc_urls)\n",
    "\n",
    "graph_index.extract_and_build(docs, show_progress=True)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a30ce502f73ac",
   "metadata": {},
   "source": [
    "## Optional - Learning how to use Readers\n",
    "### WebReaderProvider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655097e-d988-4fd8-a0f5-78a63991d202",
   "metadata": {},
   "source": [
    "### Setup Graph_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f8f86-3b22-49e7-8284-31199aeac9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from graphrag_toolkit.lexical_graph import LexicalGraphIndex, set_logging_config\n",
    "from graphrag_toolkit.lexical_graph.storage import VectorStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage import GraphStoreFactory\n",
    "from graphrag_toolkit.lexical_graph.storage.graph.neo4j_graph_store_factory import Neo4jGraphStoreFactory\n",
    "\n",
    "# Register Neo4j as the graph store backend\n",
    "GraphStoreFactory.register(Neo4jGraphStoreFactory)\n",
    "\n",
    "# Initialize graph and vector stores from environment configuration\n",
    "graph_store = GraphStoreFactory.for_graph_store(os.environ['GRAPH_STORE'])\n",
    "vector_store = VectorStoreFactory.for_vector_store(os.environ['VECTOR_STORE'])\n",
    "\n",
    "# Create the lexical graph index\n",
    "graph_index = LexicalGraphIndex(\n",
    "    graph_store,\n",
    "    vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b536ce6540fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import WebReaderProvider, WebReaderConfig\n",
    "\n",
    "\n",
    "# Configure web reader with enhanced metadata\n",
    "web_config = WebReaderConfig(\n",
    "    html_to_text=True,\n",
    "    metadata_fn=lambda url: {\n",
    "        'source': 'web',\n",
    "        'url': url,\n",
    "        'document_type': 'aws_documentation',\n",
    "        'service': 'amazon_bedrock',\n",
    "        'domain': 'aws.amazon.com'\n",
    "    }\n",
    ")\n",
    "\n",
    "web_reader = WebReaderProvider(web_config)\n",
    "\n",
    "# AWS Bedrock documentation URLs\n",
    "doc_urls = [\n",
    "    'https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html',\n",
    "    'https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html',\n",
    "    'https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html',\n",
    "    'https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation.html'\n",
    "]\n",
    "\n",
    "# Read all web documents\n",
    "all_docs = []\n",
    "for url in doc_urls:\n",
    "    docs = web_reader.read(url)\n",
    "    all_docs.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} web documents\")\n",
    "if all_docs:\n",
    "    print(f\"First document metadata: {all_docs[0].metadata}\")\n",
    "\n",
    "# Index the documents\n",
    "graph_index.extract_and_build(all_docs, show_progress=True)\n",
    "\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9efcf00729142",
   "metadata": {},
   "source": [
    "### PDFReaderProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545d57ec4aeb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import PDFReaderProvider, PDFReaderConfig\n",
    "\n",
    "# Configure PDF reader with enhanced metadata\n",
    "pdf_config = PDFReaderConfig(\n",
    "    return_full_document=False,\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'pdf',\n",
    "        'file_path': path,\n",
    "        'document_type': 'pdf_document',\n",
    "        'content_category': 'technical_documentation'\n",
    "    }\n",
    ")\n",
    "\n",
    "pdf_reader = PDFReaderProvider(pdf_config)\n",
    "\n",
    "# PDF file path\n",
    "pdf_path = \"pdf/sample.pdf\"\n",
    "\n",
    "# Read the PDF file\n",
    "pdf_docs = pdf_reader.read(pdf_path)\n",
    "print(f\"Loaded {len(pdf_docs)} PDF documents\")\n",
    "\n",
    "if pdf_docs:\n",
    "    print(f\"First document metadata: {pdf_docs[0].metadata}\")\n",
    "\n",
    "# Index the PDF documents\n",
    "graph_index.extract_and_build(pdf_docs, show_progress=True)\n",
    "\n",
    "print(\"PDF Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263e6d5edc50d5c",
   "metadata": {},
   "source": [
    "### YouTubeReaderProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d39664af5b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import YouTubeReaderProvider, YouTubeReaderConfig\n",
    "\n",
    "# Configure YouTube reader\n",
    "youtube_config = YouTubeReaderConfig(\n",
    "    language=\"en\",\n",
    "    metadata_fn=lambda url: {\n",
    "        'source': 'youtube',\n",
    "        'content_type': 'video_transcript',\n",
    "        'platform': 'youtube'\n",
    "    }\n",
    ")\n",
    "\n",
    "provider = YouTubeReaderProvider(youtube_config)\n",
    "\n",
    "# YouTube URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=YmR2_zlQO5w\"\n",
    "\n",
    "# Extract and build\n",
    "docs = provider.read(youtube_url)\n",
    "print(f\"Loaded {len(docs)} YouTube documents\")\n",
    "\n",
    "graph_index.extract_and_build(docs, show_progress=True)\n",
    "\n",
    "print(\"YouTube Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305db5092f559b18",
   "metadata": {},
   "source": [
    "### DocxReaderProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7592b75d259ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import DocxReaderProvider, DocxReaderConfig\n",
    "\n",
    "# Configure DOCX reader with enhanced metadata\n",
    "docx_config = DocxReaderConfig(\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'docx',\n",
    "        'file_path': path,\n",
    "        'document_type': 'word_document',\n",
    "        'content_category': 'office_document',\n",
    "        'file_extension': '.docx'\n",
    "    }\n",
    ")\n",
    "\n",
    "docx_reader = DocxReaderProvider(docx_config)\n",
    "\n",
    "# DOCX file path\n",
    "docx_path = \"docs/story.docx\"\n",
    "\n",
    "# Read the DOCX file\n",
    "docx_docs = docx_reader.read(docx_path)\n",
    "print(f\"Loaded {len(docx_docs)} DOCX documents\")\n",
    "\n",
    "if docx_docs:\n",
    "    print(f\"First document metadata: {docx_docs[0].metadata}\")\n",
    "\n",
    "# Index the DOCX documents\n",
    "graph_index.extract_and_build(docx_docs, show_progress=True)\n",
    "\n",
    "print(\"Microsoft Word Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af558442156bba3",
   "metadata": {},
   "source": [
    "### GithubRepositoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df760df-9fa1-447c-85fd-c4569ba2d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Fix for asyncio event loop conflict in Jupyter\n",
    "\n",
    "\n",
    "# Use the LlamaIndex GitHub reader directly\n",
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "\n",
    "# GitHub token - replace with your actual token\n",
    "github_token = \"\"  # Replace with your GitHub token\n",
    "\n",
    "if github_token and github_token != \"ghp_your_token_here\":\n",
    "    print(\"Using authenticated GitHub access with token.\")\n",
    "else:\n",
    "    print(\"No valid GITHUB_TOKEN found — using unauthenticated access. You may be rate-limited.\")\n",
    "    print(\"To add a token:\")\n",
    "    print(\"Replace 'ghp_your_token_here' with your actual GitHub personal access token\")\n",
    "\n",
    "# Create GitHub client and reader\n",
    "github_client = GithubClient(github_token=github_token, verbose=True)\n",
    "reader = GithubRepositoryReader(\n",
    "    github_client=github_client,\n",
    "    owner=\"evanerwee\",\n",
    "    repo=\"graphrag-toolkit\",\n",
    "    use_parser=False,\n",
    "    verbose=False,\n",
    "    filter_directories=(\n",
    "        [\"docs\"],  # Only read specific directories\n",
    "        GithubRepositoryReader.FilterType.INCLUDE,\n",
    "    ),\n",
    "    filter_file_extensions=(\n",
    "        [\".md\", \".py\", \".txt\", \".rst\"],  # Only read specific file types\n",
    "        GithubRepositoryReader.FilterType.INCLUDE,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Load documents from the main branch\n",
    "print(\"Loading GitHub repository documents...\")\n",
    "github_docs = reader.load_data(branch=\"main\")\n",
    "\n",
    "print(f\"Loaded {len(github_docs)} GitHub documents\")\n",
    "\n",
    "if github_docs:\n",
    "    print(f\"First document metadata: {github_docs[0].metadata}\")\n",
    "    \n",
    "    # Add enhanced metadata\n",
    "    for doc in github_docs:\n",
    "        doc.metadata.update({\n",
    "            'source': 'github',\n",
    "            'repository': 'awslabs/graphrag-toolkit',\n",
    "            'document_type': 'source_code',\n",
    "            'content_category': 'repository_content',\n",
    "            'platform': 'github'\n",
    "        })\n",
    "\n",
    "# Index the GitHub documents\n",
    "graph_index.extract_and_build(github_docs, show_progress=True)\n",
    "\n",
    "print(\"GitHub Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e6bafcdbda74e",
   "metadata": {},
   "source": [
    "### PPTXReaderProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd693512c1bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import PPTXReaderProvider, PPTXReaderConfig\n",
    "\n",
    "# Configure PPTX reader with enhanced metadata\n",
    "pptx_config = PPTXReaderConfig(\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'pptx',\n",
    "        'file_path': path,\n",
    "        'document_type': 'powerpoint_presentation',\n",
    "        'content_category': 'office_document',\n",
    "        'file_extension': '.pptx'\n",
    "    }\n",
    ")\n",
    "\n",
    "pptx_reader = PPTXReaderProvider(pptx_config)\n",
    "\n",
    "# PPTX file path\n",
    "pptx_path = \"pptx/sample.pptx\"\n",
    "\n",
    "# Read the PPTX file\n",
    "pptx_docs = pptx_reader.read(pptx_path)\n",
    "print(f\"Loaded {len(pptx_docs)} PPTX documents\")\n",
    "\n",
    "if pptx_docs:\n",
    "    print(f\"First document metadata: {pptx_docs[0].metadata}\")\n",
    "\n",
    "# Index the PPTX documents\n",
    "graph_index.extract_and_build(pptx_docs, show_progress=True)\n",
    "\n",
    "print(\"PowerPoint Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768c34e-f24d-4d40-9aab-4a743ac47d7b",
   "metadata": {},
   "source": [
    "### Markdown Reader Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050ba67-1be9-4f4b-b841-f654b5522ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import MarkdownReaderProvider, MarkdownReaderConfig\n",
    "\n",
    "# Configure Markdown reader\n",
    "md_config = MarkdownReaderConfig(\n",
    "    remove_hyperlinks=True,\n",
    "    remove_images=True,\n",
    "    metadata_fn=lambda path: {'source': 'markdown', 'file_path': path}\n",
    ")\n",
    "\n",
    "md_reader = MarkdownReaderProvider(md_config)\n",
    "\n",
    "# Read your actual Markdown file\n",
    "md_docs = md_reader.read('artifacts/sample.md')\n",
    "\n",
    "print(f\"Loaded {len(md_docs)} Markdown documents\")\n",
    "print(f\"Document content preview: {md_docs[0].text[:200]}...\")\n",
    "\n",
    "# Index the documents\n",
    "graph_index.extract_and_build(md_docs, show_progress=True)\n",
    "\n",
    "print(\"Markdown Extraction Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b6d7c-0a9e-4fd5-860e-e83598ff61b8",
   "metadata": {},
   "source": [
    "### JSON Reader Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344bad4-0afc-4261-a933-8b266cb060ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cache directories and environment\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "# Set environment variables for writable cache directories\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = '/home/jovyan/work/.cache/llama_index'\n",
    "os.environ['NLTK_DATA'] = '/home/jovyan/work/nltk_data'\n",
    "\n",
    "# Create cache directories\n",
    "cache_dir = '/home/jovyan/work/.cache/llama_index'\n",
    "nltk_dir = '/home/jovyan/work/nltk_data'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.makedirs(nltk_dir, exist_ok=True)\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.data.path.append(nltk_dir)\n",
    "nltk.download('punkt', download_dir=nltk_dir)\n",
    "\n",
    "# Import and configure JSON reader\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import JSONReaderProvider, JSONReaderConfig\n",
    "\n",
    "# Configure JSON reader\n",
    "json_config = JSONReaderConfig(\n",
    "    metadata_fn=lambda path: {'source': 'json', 'file_path': path}\n",
    ")\n",
    "\n",
    "json_reader = JSONReaderProvider(json_config)\n",
    "\n",
    "# Read your actual JSON file\n",
    "json_docs = json_reader.read('artifacts/sample.json')\n",
    "\n",
    "print(f\"Loaded {len(json_docs)} JSON documents\")\n",
    "print(f\"Document content preview: {json_docs[0].text[:200]}...\")\n",
    "\n",
    "# Index the documents\n",
    "graph_index.extract_and_build(json_docs, show_progress=True)\n",
    "\n",
    "print(\"JSON Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815a810-e587-47b2-b236-e73184e1946a",
   "metadata": {},
   "source": [
    "### Wikipedia Reader Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae57772-fd39-4224-9fea-db37d71e8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import WikipediaReaderProvider, WikipediaReaderConfig\n",
    "import wikipedia\n",
    "\n",
    "# Configure Wikipedia reader\n",
    "wiki_config = WikipediaReaderConfig(\n",
    "    lang='en',\n",
    "    metadata_fn=lambda title: {'source': 'wikipedia', 'title': title, 'language': 'en'}\n",
    ")\n",
    "\n",
    "wiki_reader = WikipediaReaderProvider(wiki_config)\n",
    "\n",
    "# Search and validate the Wikipedia page\n",
    "search_results = wikipedia.search('Llama language model')\n",
    "print(f\"Search results: {search_results}\")\n",
    "\n",
    "# Validate the page exists\n",
    "page_title = None\n",
    "for result in search_results:\n",
    "    try:\n",
    "        wikipedia.page(result)\n",
    "        page_title = result\n",
    "        break\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        continue\n",
    "\n",
    "# Fallback to a known valid page\n",
    "if not page_title:\n",
    "    page_title = 'Large language model'\n",
    "\n",
    "print(f\"Using page: {page_title}\")\n",
    "\n",
    "# Read Wikipedia article\n",
    "wiki_docs = wiki_reader.read(page_title)\n",
    "\n",
    "print(f\"Loaded {len(wiki_docs)} Wikipedia documents\")\n",
    "print(f\"Document content preview: {wiki_docs[0].text[:200]}...\")\n",
    "print(f\"Document metadata: {wiki_docs[0].metadata}\")\n",
    "\n",
    "# Index the documents\n",
    "graph_index.extract_and_build(wiki_docs, show_progress=True)\n",
    "\n",
    "print(\"Wikipedia Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d5eb5-9e2f-43f6-b91a-707d28a69ffd",
   "metadata": {},
   "source": [
    "### CSV Reader Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273291fb-3657-4a27-9392-ab915238ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import CSVReaderProvider, CSVReaderConfig\n",
    "\n",
    "# Configure CSV reader\n",
    "csv_config = CSVReaderConfig(\n",
    "    concat_rows=True,\n",
    "    metadata_fn=lambda path: {'source': 'csv', 'file_path': path}\n",
    ")\n",
    "\n",
    "csv_reader = CSVReaderProvider(csv_config)\n",
    "\n",
    "# Read your actual CSV file\n",
    "csv_docs = csv_reader.read('artifacts/sample.csv')\n",
    "\n",
    "print(f\"Loaded {len(csv_docs)} CSV documents\")\n",
    "print(f\"Document content preview: {csv_docs[0].text[:200]}...\")\n",
    "\n",
    "# Index the documents\n",
    "graph_index.extract_and_build(csv_docs, show_progress=True)\n",
    "\n",
    "print(\"CSV Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8482ec-4696-4ec3-8472-6431f078c165",
   "metadata": {},
   "source": [
    "## Complete\n",
    "\n",
    "This notebook demonstrated various reader providers available in the GraphRAG Toolkit:\n",
    "\n",
    "- **WebReaderProvider**: For reading web pages\n",
    "- **PDFReaderProvider**: For reading PDF documents\n",
    "- **CSVReaderProvider**: For reading CSV files\n",
    "- **JSONReaderProvider**: For reading JSON files\n",
    "- **MarkdownReaderProvider**: For reading Markdown files\n",
    "- **WikipediaReaderProvider**: For reading Wikipedia articles\n",
    "- **YouTubeReaderProvider**: For reading YouTube\n",
    "- **DocxReaderProvider**: For reading Microsoft Word\n",
    "- **GithubRepositoryReader**: For reading Github repository\n",
    "- **PPTXReaderProvider**: For reading Microsoft Powerpoint\n",
    "\n",
    "Each provider can be configured with specific options and metadata functions to customize the document loading process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd28de5-a1a5-49d8-84b5-e9273ed2d978",
   "metadata": {},
   "source": [
    "### Readers for Structured-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0ff98-114a-4064-a6f1-4c6bc8c9314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import StructuredDataReaderProvider, StructuredDataReaderConfig\n",
    "\n",
    "# Configure structured data reader with enhanced metadata\n",
    "structured_config = StructuredDataReaderConfig(\n",
    "    col_index=0,  # Column to use as index (0 = first column)\n",
    "    col_joiner=', ',  # String to join multiple columns\n",
    "    col_metadata=None,  # Optional column metadata configuration\n",
    "    pandas_config={\"sep\": \",\"},  # CSV separator and other pandas options\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'structured_data',\n",
    "        'file_path': path,\n",
    "        'document_type': 'structured_data',\n",
    "        'content_category': 'tabular_data'\n",
    "    }\n",
    ")\n",
    "\n",
    "structured_reader = StructuredDataReaderProvider(structured_config)\n",
    "\n",
    "# Read CSV and Excel files (supports both local files and S3 URLs)\n",
    "data_files = [\"artifacts/sample.csv\", \"artifacts/sample.xlsx\"]\n",
    "structured_docs = structured_reader.read(data_files)\n",
    "\n",
    "print(f\"Loaded {len(structured_docs)} structured data documents\")\n",
    "\n",
    "if structured_docs:\n",
    "    print(f\"First document metadata: {structured_docs[0].metadata}\")\n",
    "\n",
    "# Index the structured data documents\n",
    "graph_index.extract_and_build(structured_docs, show_progress=True)\n",
    "\n",
    "print(\"Structured Data Extraction Complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea9d67d42d20e3",
   "metadata": {},
   "source": [
    "### DirectoryReaderProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6216f7-a3bf-420a-865e-69e2293e8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.lexical_graph.indexing.load.readers import DirectoryReaderProvider, DirectoryReaderConfig\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create test directory structure\n",
    "test_dir = Path('dir_reader')\n",
    "test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample files\n",
    "sample_files = {\n",
    "    'document1.txt': 'This is the first document. It contains information about artificial intelligence and machine learning.',\n",
    "    'document2.txt': 'This is the second document. It discusses natural language processing and deep learning techniques.',\n",
    "    'notes.md': '# Meeting Notes\\n\\n## Key Points\\n- Discussed project timeline\\n- Reviewed technical requirements\\n- Planned next steps',\n",
    "    'data.json': '{\"name\": \"Sample Data\", \"type\": \"test\", \"values\": [1, 2, 3, 4, 5]}'\n",
    "}\n",
    "\n",
    "for filename, content in sample_files.items():\n",
    "    (test_dir / filename).write_text(content)\n",
    "\n",
    "print(f\"Created test directory '{test_dir}' with {len(sample_files)} files\")\n",
    "print(f\"Files: {list(sample_files.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654aaec9-9be0-41a5-9403-a067a46310be",
   "metadata": {},
   "source": [
    "#### Basic Directory Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32fd44-6ecc-4545-a5c5-7d895c17a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure directory reader for all files\n",
    "dir_config = DirectoryReaderConfig(\n",
    "    input_dir=str(test_dir),\n",
    "    exclude_hidden=True,\n",
    "    recursive=True,\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'directory',\n",
    "        'directory_path': path,\n",
    "        'reader_type': 'basic'\n",
    "    }\n",
    ")\n",
    "\n",
    "dir_reader = DirectoryReaderProvider(dir_config)\n",
    "\n",
    "# Read all documents from directory\n",
    "dir_docs = dir_reader.read(None)\n",
    "\n",
    "print(f\"Loaded {len(dir_docs)} documents from directory\")\n",
    "print(\"\\nDocument details:\")\n",
    "for i, doc in enumerate(dir_docs):\n",
    "    file_name = doc.metadata.get('file_name', 'unknown')\n",
    "    file_path = doc.metadata.get('file_path', 'unknown')\n",
    "    print(f\"  Document {i+1}: {file_name}\")\n",
    "    print(f\"    Path: {file_path}\")\n",
    "    print(f\"    Content preview: {doc.text[:100]}...\")\n",
    "    print(f\"    Custom metadata: {doc.metadata.get('reader_type')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021cf747-3c32-458b-b265-333107698e87",
   "metadata": {},
   "source": [
    "#### Filtered Directory Reading\n",
    "\n",
    "Read only specific file types from the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219f79d-09ed-41af-8d5f-71bf4fd44f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure directory reader for text files only\n",
    "filtered_config = DirectoryReaderConfig(\n",
    "    input_dir=str(test_dir),\n",
    "    exclude_hidden=True,\n",
    "    recursive=True,\n",
    "    required_exts=[\".txt\", \".md\"],  # Only read .txt and .md files\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'directory_filtered',\n",
    "        'directory_path': path,\n",
    "        'filter': 'text_files_only'\n",
    "    }\n",
    ")\n",
    "\n",
    "filtered_reader = DirectoryReaderProvider(filtered_config)\n",
    "\n",
    "# Read filtered documents\n",
    "filtered_docs = filtered_reader.read(None)\n",
    "\n",
    "print(f\"Loaded {len(filtered_docs)} filtered documents (txt and md only)\")\n",
    "print(\"\\nFiltered document details:\")\n",
    "for i, doc in enumerate(filtered_docs):\n",
    "    file_name = doc.metadata.get('file_name', 'unknown')\n",
    "    file_ext = Path(file_name).suffix\n",
    "    print(f\"  Document {i+1}: {file_name} ({file_ext})\")\n",
    "    print(f\"    Content preview: {doc.text[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3380d1f-4c18-46df-bf81-f547958cefbe",
   "metadata": {},
   "source": [
    "#### Nested Directory Structure\n",
    "\n",
    "Test recursive reading with nested directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3f59d-f1c9-49eb-b305-312aa4e06d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested directory structure\n",
    "nested_dir = test_dir / 'subdirectory'\n",
    "nested_dir.mkdir(exist_ok=True)\n",
    "\n",
    "deep_dir = nested_dir / 'deep'\n",
    "deep_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Add files to nested directories\n",
    "nested_files = {\n",
    "    nested_dir / 'nested_doc.txt': 'This document is in a subdirectory. It contains nested content.',\n",
    "    deep_dir / 'deep_doc.txt': 'This document is deeply nested. It demonstrates recursive directory reading.'\n",
    "}\n",
    "\n",
    "for file_path, content in nested_files.items():\n",
    "    file_path.write_text(content)\n",
    "\n",
    "print(f\"Created nested structure:\")\n",
    "print(f\"  {nested_dir}/nested_doc.txt\")\n",
    "print(f\"  {deep_dir}/deep_doc.txt\")\n",
    "\n",
    "# Configure recursive directory reader\n",
    "recursive_config = DirectoryReaderConfig(\n",
    "    input_dir=str(test_dir),\n",
    "    exclude_hidden=True,\n",
    "    recursive=True,  # Enable recursive reading\n",
    "    required_exts=[\".txt\"],\n",
    "    metadata_fn=lambda path: {\n",
    "        'source': 'directory_recursive',\n",
    "        'directory_path': path,\n",
    "        'scan_type': 'recursive'\n",
    "    }\n",
    ")\n",
    "\n",
    "recursive_reader = DirectoryReaderProvider(recursive_config)\n",
    "\n",
    "# Read all txt files recursively\n",
    "recursive_docs = recursive_reader.read(None)\n",
    "\n",
    "print(f\"\\nLoaded {len(recursive_docs)} documents recursively\")\n",
    "print(\"\\nRecursive document details:\")\n",
    "for i, doc in enumerate(recursive_docs):\n",
    "    file_path = doc.metadata.get('file_path', 'unknown')\n",
    "    file_name = doc.metadata.get('file_name', 'unknown')\n",
    "    print(f\"  Document {i+1}: {file_name}\")\n",
    "    print(f\"    Full path: {file_path}\")\n",
    "    print(f\"    Content preview: {doc.text[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a487d1-8d6d-441c-a805-dd5d5f065451",
   "metadata": {},
   "source": [
    "#### Index Directory Documents\n",
    "\n",
    "Index all the documents we've read from the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56fec1-897d-416f-94e0-44247962f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents for indexing\n",
    "all_directory_docs = dir_docs + filtered_docs + recursive_docs\n",
    "\n",
    "print(f\"Total documents to index: {len(all_directory_docs)}\")\n",
    "\n",
    "# Show document sources\n",
    "sources = {}\n",
    "for doc in all_directory_docs:\n",
    "    source = doc.metadata.get('source', 'unknown')\n",
    "    sources[source] = sources.get(source, 0) + 1\n",
    "\n",
    "print(\"\\nDocument sources:\")\n",
    "for source, count in sources.items():\n",
    "    print(f\"  {source}: {count} documents\")\n",
    "\n",
    "# Index the documents\n",
    "print(\"\\nIndexing directory documents...\")\n",
    "graph_index.extract_and_build(all_directory_docs, show_progress=True)\n",
    "\n",
    "print(\"Directory documents indexed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
