{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad881f1",
   "metadata": {},
   "source": [
    "# BYOKG RAG using Neptune Analytics as GraphStore and Vector Store \n",
    "This notebook demonstrates a RAG (Retrieval Augmented Generation) system built on top of a Knowledge Graph. In this example, we demonstrate how text embeddings can be used within the BYOKG framework and use a Neptune Analytics graph as the graphstore and the vector store for embeddings. The overall system allows querying a knowledge graph using natural language questions and retrieving relevant information to generate answers.\n",
    "\n",
    "1. **Graph Store**: Neptune Analytics endpoint for the graph structure and for storing embeddings based on the graph\n",
    "2. **KG Linker**: Links natural language queries to graph entities and paths\n",
    "3. **Entity Linker**: Matches entities from question text to graph nodes\n",
    "6. **Query Engine**: Orchestrates all components to answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71f996",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "If you haven't already, install the toolkit and dependencies in [README.md](../../byokg-rag/README.md).\n",
    "Let's validate if the package is correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdac31-35e0-4cd3-9456-2d7ab58a172e",
   "metadata": {},
   "outputs": [],
   "source": "# !pip install https://github.com/awslabs/graphrag-toolkit/archive/refs/tags/v3.12.0.zip#subdirectory=byokg-rag"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceff478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.byokg_rag.graphstore import NeptuneAnalyticsGraphStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d4754e",
   "metadata": {},
   "source": [
    "### Graph Store\n",
    "The `NeptuneAnalyticsGraphStore` class provides an interface to work with the Neptune Analytics graph.\n",
    "If you already have a NeptuneAnalyticsGraphEndpoint you want to use, simply change the cell below to assign `graph_identifier` to your NeptuneAnalytics graph id. \n",
    "\n",
    "If you don't already have a Neptune Graph then you can create one by running the command below from an environment that has the AWS CLI configured with appropriate permissions. Please refer to documentation for more details about [creating a graph](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/create-graph-using-console.html) and [loading data into the graph](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/batch-load.html).\n",
    "\n",
    "```\n",
    "aws neptune-graph create-graph --graph-name 'test-kg-with-embedding' --provisioned-memory 128 --public-connectivity --replica-count 0 --vector-search-configuration '{\"dimension\": 1024}'\n",
    "```\n",
    "\n",
    "After running the command you should receive a response that includes the graph id. Change the cell below to assign  `graph_identifier` to the id.\n",
    "\n",
    "To run the rest of the notebook, you'll need to ensure that the environment has the right IAM permissions to interact with your neptune analytics graph endpoint. Specifically you will need `neptune-graph:ReadDataViaQuery` and `neptune-graph:GetGraph`. You will also need s3 IAM read permissions so that `graphstore.read_from_csv` can access data from `s3://aws-neptune-customer-samples-*/*` and optionally, s3 IAM read and write permissions to your s3 bucket so that embeddings can be saved and loaded from your desired s3 location.\n",
    "\n",
    "In the rest of the notebook, we\n",
    "1. Initialize the BYOKG graph store to use a Neptune Analytics Graph\n",
    "2. Optionally, load an example data from a CSV file for a new graph and get basic statistics\n",
    "3. Demonstrate using local embedding models and a local vector store how embeddings are generated and used for retrieval and linking.\n",
    "4. Finally, combine all the steps using the NeptuneAnalyticsGraphStore and BYOKGQueryEngine to combine all the steps into a RAG pipeline and answer a sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28739bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\" #replace with aws region\n",
    "graph_identifier = \"<>\" # replace with graph id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = NeptuneAnalyticsGraphStore(graph_identifier=graph_identifier,\n",
    "                                         region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902a8cd7",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "\n",
    "If you ran the command to create a new graph, then uncomment the code cell below to load the new graph with some data. The data we are loading is a KG with information about AWS blog posts on Neptune and Neptune Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_store.read_from_csv(s3_path=f\"s3://aws-neptune-customer-samples-{region}/sample-datasets/gremlin/KG/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee946d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print graph statistics\n",
    "number_of_nodes = len(graph_store.nodes())\n",
    "number_of_edges = len(graph_store.edges())\n",
    "print(f\"The graph has {number_of_nodes} nodes and {number_of_edges} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3005cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print graph schema\n",
    "import json\n",
    "\n",
    "schema = graph_store.get_schema()\n",
    "print(json.dumps(schema, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3296c7",
   "metadata": {},
   "source": [
    "\n",
    "### Node Textual Representation for Embedding and  Vector Index\n",
    "\n",
    "Now that we have seen the graph schema, we can start to assign which properties will be useful in generating a text representation for each node. Below we create a dictionary where each key is a node label or node type and the corresponding values are the properties to use to represent that node. Only the nodes belonging to node labels in the dictionary will have text representations for embeddings. To use all properties, set `node_embedding_text_properties` to \"ALL_PROPERTIES\". The final text representation of each node is a stringified json of the property keys and their values for that node as shown in the output of cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31415ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embedding_text_properties = {\n",
    "    \"organization\": [\"type\", \"text\"],\n",
    "    \"author\": [\"name\"],\n",
    "    \"title\": [\"text\"],\n",
    "    \"commercial_item\": [\"type\", \"text\"],\n",
    "    \"tag\": [\"tag\"],\n",
    "    \"location\": [\"type\", \"text\"],\n",
    "    \"post\": [\"title\"],\n",
    "    \"date\": [\"type\", \"text\"]\n",
    "}\n",
    "node_ids, texts_to_embed = graph_store.get_node_text_for_embedding_input(node_embedding_text_properties)\n",
    "\n",
    "print(node_ids[:3])\n",
    "print(texts_to_embed[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ba671",
   "metadata": {},
   "source": [
    "#### Local embedding model and index\n",
    "Once we have texts to embed then we can create an embedding model and use that to generate an embeddings for this node. To illustrate in some details how the embedding model and embedding index, we will use an embedding model that can be run locally via langchain_hugging_face and we will use a local dense index or vector store. The vector store is based on the faiss library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd427c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.byokg_rag.indexing import LocalFaissDenseIndex, HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\",\n",
    "                                       model_kwargs={\"device\":\"cpu\"}, #change to 'cuda' if gpu is available\n",
    "                                       encode_kwargs={\"batch_size\": 8},\n",
    "                                       multi_process=True,\n",
    "                                       show_progress=True)\n",
    "\n",
    "create_index_args = {\"embedding\": embedding_model, \"distance_type\":\"inner_product\", \"embedding_dim\": 1024}\n",
    "\n",
    "faiss_index = LocalFaissDenseIndex(**create_index_args)\n",
    "faiss_index.add_with_ids(node_ids, texts_to_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c6e1e",
   "metadata": {},
   "source": [
    "Now let's test retrieval directly from the vector store with a few questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a3874",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = \"Did any posts show case a media and entertainment use case?\"\n",
    "response = faiss_index.query(input_question, topk=3)\n",
    "print(response['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_question = \"Did any posts show case a migration use case?\"\n",
    "response = faiss_index.query(input_question, topk=3)\n",
    "print(response['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfc82c",
   "metadata": {},
   "source": [
    "The `EntityLinker` uses query or extracted entities to match against node embeddings saved in the embedding index.\n",
    "\n",
    "To use this we convert our index to an entity matcher which we then use to initialize an EntityLinker. This will return just node ids in the graph that are relevant to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde72518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.byokg_rag.graph_retrievers import EntityLinker\n",
    "entity_linker = EntityLinker(retriever=faiss_index.as_entity_matcher())\n",
    "\n",
    "linked_entities = entity_linker.link([input_question], return_dict=False)\n",
    "print(linked_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ead8540",
   "metadata": {},
   "source": [
    "### Neptune Analytics GraphStore Embedding Index\n",
    "\n",
    "Now that we have validated and examined how text to be embedded is prepared, how the embedding is generated and used for entity linking, let's examine how to put it all together and use managed APIs for embedding generation and Neptune Analytics as the vector store to store the embeddings.\n",
    "\n",
    "The NeptuneAnalyticsGraphStore class has a convenient `.as_embedding_index` function which can accepts a `graphrag_toolkit.byokg_rag.indexing.Embedding` object and dictionary containing the properties to use to generate the text input to the embeddings. Specifying `load=True` in this function means that graphstore will generate the node text for each node, compute the embeddings and save the embeddings as a vector in the Neptune Analytics graph while `load=False` just directly returns an index object for retrieval. This is useful is the graph already contains embeddings. You can also pass in `embedding_s3_save_location` which is a s3 file location that will be used to load the embeddings from if the already exists, but if the file doesn't exist then it will be created and the computed will also be stored in that s3 file.\n",
    "\n",
    "In this cell, we use the cohere embedding model on Bedrock which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e81de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphrag_toolkit.byokg_rag.indexing import LLamaIndexBedrockEmbedding\n",
    "from graphrag_toolkit.byokg_rag.graph_retrievers import EntityLinker\n",
    "\n",
    "index = graph_store.as_embedding_index(embedding=LLamaIndexBedrockEmbedding(model_name=\"cohere.embed-english-v3\",\n",
    "                                                                            region_name=region),\n",
    "                                       node_embedding_text_props=node_embedding_text_properties,\n",
    "                                       load=True\n",
    "                                      )\n",
    "\n",
    "entity_linker = EntityLinker(retriever=index.as_entity_matcher())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5307b",
   "metadata": {},
   "source": [
    "### BYOKG RAG Pipeline for QA with Neptune Analytics Embedding\n",
    "\n",
    "Now let's use the `ByoKGQueryEngine` to combine create a question answering pipeline with our graphstore and embedding index for entity linking. To get more details about the different graph retrievers in `ByoKGQueryEngine`, see the `byokg_rag_neptune_analytics_demo.ipynb` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set node property for graph store and graph traversal to use to understand and verbalize each node ids\n",
    "text_repr_prop_for_node = {\n",
    "    \"organization\": \"text\",\n",
    "    \"author\": \"name\",\n",
    "    \"title\": \"text\",\n",
    "    \"commercial_item\": \"text\",\n",
    "    \"tag\": \"tag\",\n",
    "    \"location\": \"text\",\n",
    "    \"post\": \"title\",\n",
    "}\n",
    "graph_store.assign_text_repr_prop_for_nodes(text_repr_prop_for_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a question to test queries\n",
    "question = \"Who is the author of post on migrating from blazegraph to amazon neptune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac11cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and run query engine\n",
    "\n",
    "from graphrag_toolkit.byokg_rag.byokg_query_engine import ByoKGQueryEngine\n",
    "\n",
    "byokg_query_engine = ByoKGQueryEngine(\n",
    "    graph_store=graph_store,\n",
    "    entity_linker=entity_linker,\n",
    "    direct_query_linking=True,\n",
    ")\n",
    "\n",
    "retrieved_context = byokg_query_engine.query(question)\n",
    "answers, response = byokg_query_engine.generate_response(question, \"\\n\".join(retrieved_context))\n",
    "\n",
    "print(retrieved_context)\n",
    "print(answers)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
